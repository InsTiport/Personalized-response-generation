import os
import numpy as np
import re
from pysbd import Segmenter
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm


def get_wiki(wiki_idx):
    with open(os.path.join('data', 'wiki', wiki_idx), 'r') as read_wiki:
        wiki = read_wiki.read()
    return wiki


def select_most_similar(queries, text, top_l=3):
    text = re.sub(r'\n+', '\n', text)
    text = text.replace('\n', ' ')

    seg = Segmenter(language='en', clean=False)
    sentences = list(seg.segment(text))
    sentences = [s.strip() for s in sentences]

    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(queries + sentences).toarray()

    scores = vectors[:len(queries), :] @ vectors[len(queries):, :].T
    indices = scores.argsort(axis=1)[:, -min(top_l, len(sentences)):]

    # res will be a list of strings generated by concatenating all l most similar sentences for each query
    res = []
    for indices_for_one_query in indices:
        res.append(' '.join([sentences[index] for index in range(len(sentences)) if index in indices_for_one_query]))
    return res


os.chdir('../')

print('generating tsv file...')
with open(os.path.join('data', 'interview.txt'), 'r') as r:
    dataset = r.read()
    dataset = [interview for interview in dataset.split('[SEP]') if len(interview) > 10]  # don't include last one (\n)

    with open(os.path.join('data', 'interview_qa.tsv'), 'w') as w:
        w.write('id\t')
        w.write('sport_type\t')
        w.write('game_wiki_id\t')
        w.write('game_wiki\t')
        w.write('section_wiki_id\t')
        w.write('section_wiki\t')
        w.write('title\t')
        w.write('date\t')
        w.write('participants\t')
        w.write('background\t')
        w.write('respondent\t')
        w.write('question\t')
        w.write('response\n')

        player2index = dict()

        for interview in tqdm(dataset):
            lines = interview.split('\n')
            # some lines may be empty due to splitting, remove those
            lines = [line.replace('End of FastScripts', '').strip() for line in lines if len(line) > 3]

            interview_id = lines[0][lines[0].index('[id]') + len('[id] '):]
            sport_type = lines[1][lines[1].index('[sport_type]') + len('[sport_type] '):]

            game_wiki_id = lines[2][lines[2].index('[game_wiki]') + len('[game_wiki] '):]
            section_wiki_id = lines[3][lines[3].index('[section_wiki]') + len('[section_wiki] '):]

            title = lines[4][lines[4].index('[title]') + len('[title] '):]
            date = lines[5][lines[5].index('[date]') + len('[date] '):]
            participants = lines[6][lines[6].index('[participants]') + len('[participants] '):]

            background = ''
            question_list = []
            response_list = []
            respondent_list = []
            for i in range(7, len(lines)):
                if '[background]' in lines[i]:
                    background += lines[i][lines[i].index('[background]') + len('[background] '):] + ' '
                elif '[QA]' in lines[i]:
                    if len(lines[i].split('\t')) < 2:  # some questions are thank-yous and do not have a response
                        continue

                    question, response = lines[i].split('\t')[:2]  # only keep the first response for every question
                    question = question[question.index('Q:') + len('Q: '):]

                    respondent = response[:response.index(':')].lower().title()
                    respondent_with_sport_type = respondent + '_' + sport_type
                    if respondent_with_sport_type not in player2index.keys():
                        player2index[respondent_with_sport_type] = len(player2index)
                    response = response[response.index(':') + len(': '):]
                    if len(question) == 0 or len(response) == 0:
                        continue

                    question_list.append(question)
                    response_list.append(response)
                    respondent_list.append(respondent + ' | ' + str(player2index[respondent_with_sport_type]))

            if game_wiki_id != '':
                game_wikis = select_most_similar(question_list, get_wiki(game_wiki_id))
            else:
                game_wikis = [''] * len(question_list)

            if section_wiki_id != '':
                section_wikis = select_most_similar(question_list, get_wiki(section_wiki_id))
            else:
                section_wikis = [''] * len(question_list)

            interview_id_list = [interview_id] * len(question_list)
            sport_type_list = [sport_type] * len(question_list)
            game_wiki_id_list = [game_wiki_id] * len(question_list)
            section_wiki_id_list = [section_wiki_id] * len(question_list)
            title_list = [title] * len(question_list)
            date_list = [date] * len(question_list)
            participants_list = [participants] * len(question_list)
            background_list = [background] * len(question_list)

            for interview_id, sport_type, game_wiki_id, game_wiki, section_wiki_id, section_wiki, title, date, \
                participants, background, respondent, question, response in zip(
                        interview_id_list,
                        sport_type_list,
                        game_wiki_id_list,
                        game_wikis,
                        section_wiki_id_list,
                        section_wikis,
                        title_list,
                        date_list,
                        participants_list,
                        background_list,
                        respondent_list,
                        question_list,
                        response_list
                    ):
                w.write(f'{interview_id}\t')
                w.write(f'{sport_type}\t')
                w.write(f'{game_wiki_id}\t')
                w.write(f'{game_wiki}\t')
                w.write(f'{section_wiki_id}\t')
                w.write(f'{section_wiki}\t')
                w.write(f'{title}\t')
                w.write(f'{date}\t')
                w.write(f'{participants}\t')
                w.write(f'{background}\t')
                w.write(f'{respondent}\t')
                w.write(f'{question}\t')
                w.write(f'{response}\n')

        print(f'There are {len(player2index)} unique players.')

print('generating train, dev and test splits...')
with open(os.path.join('data', 'interview_qa.tsv'), 'r') as r:
    header = r.readline()
    lines = r.read()
    lines = [line for line in lines.split('\n') if len(line) > 3]

    shuffle_indices = np.random.choice(len(lines), len(lines), replace=False)

    idx = 0
    for split, percentage in zip(['train', 'dev', 'test'], [0.98, 0.99, 1]):
        with open(os.path.join('data', f'interview_qa_{split}.tsv'), 'w') as w:
            w.write(f'{header}')
            while idx < percentage * len(lines):
                w.write(f'{lines[shuffle_indices[idx]]}\n')
                idx += 1
